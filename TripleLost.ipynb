{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cu126\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-lightning\n",
      "  Using cached pytorch_lightning-2.5.6-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: torch>=2.1.0 in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from pytorch-lightning) (2.9.1+cu126)\n",
      "Collecting tqdm>=4.57.0 (from pytorch-lightning)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting PyYAML>5.4 (from pytorch-lightning)\n",
      "  Using cached pyyaml-6.0.3-cp310-cp310-win_amd64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.9.0)\n",
      "Collecting torchmetrics>0.7.0 (from pytorch-lightning)\n",
      "  Using cached torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from pytorch-lightning) (25.0)\n",
      "Requirement already satisfied: typing-extensions>4.5.0 in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from pytorch-lightning) (4.15.0)\n",
      "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
      "  Using cached lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=2022.5.0->pytorch-lightning)\n",
      "  Downloading aiohttp-3.13.2-cp310-cp310-win_amd64.whl.metadata (8.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
      "  Using cached frozenlist-1.8.0-cp310-cp310-win_amd64.whl.metadata (21 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
      "  Using cached multidict-6.7.0-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
      "  Downloading propcache-0.4.1-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
      "  Downloading yarl-1.22.0-cp310-cp310-win_amd64.whl.metadata (77 kB)\n",
      "Collecting idna>=2.0 (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (78.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (3.19.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from torchmetrics>0.7.0->pytorch-lightning) (2.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from tqdm>=4.57.0->pytorch-lightning) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (2.1.5)\n",
      "Using cached pytorch_lightning-2.5.6-py3-none-any.whl (831 kB)\n",
      "Downloading aiohttp-3.13.2-cp310-cp310-win_amd64.whl (455 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading multidict-6.7.0-cp310-cp310-win_amd64.whl (46 kB)\n",
      "Downloading yarl-1.22.0-cp310-cp310-win_amd64.whl (86 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Downloading frozenlist-1.8.0-cp310-cp310-win_amd64.whl (43 kB)\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
      "Downloading propcache-0.4.1-cp310-cp310-win_amd64.whl (41 kB)\n",
      "Downloading pyyaml-6.0.3-cp310-cp310-win_amd64.whl (158 kB)\n",
      "Using cached torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, PyYAML, propcache, multidict, lightning-utilities, idna, frozenlist, attrs, async-timeout, aiohappyeyeballs, yarl, aiosignal, torchmetrics, aiohttp, pytorch-lightning\n",
      "\n",
      "   -- -------------------------------------  1/15 [PyYAML]\n",
      "   ---------- -----------------------------  4/15 [lightning-utilities]\n",
      "   ------------- --------------------------  5/15 [idna]\n",
      "   ------------------ ---------------------  7/15 [attrs]\n",
      "   ------------------------ ---------------  9/15 [aiohappyeyeballs]\n",
      "   -------------------------------- ------- 12/15 [torchmetrics]\n",
      "   -------------------------------- ------- 12/15 [torchmetrics]\n",
      "   -------------------------------- ------- 12/15 [torchmetrics]\n",
      "   -------------------------------- ------- 12/15 [torchmetrics]\n",
      "   -------------------------------- ------- 12/15 [torchmetrics]\n",
      "   -------------------------------- ------- 12/15 [torchmetrics]\n",
      "   -------------------------------- ------- 12/15 [torchmetrics]\n",
      "   -------------------------------- ------- 12/15 [torchmetrics]\n",
      "   ---------------------------------- ----- 13/15 [aiohttp]\n",
      "   ------------------------------------- -- 14/15 [pytorch-lightning]\n",
      "   ------------------------------------- -- 14/15 [pytorch-lightning]\n",
      "   ------------------------------------- -- 14/15 [pytorch-lightning]\n",
      "   ------------------------------------- -- 14/15 [pytorch-lightning]\n",
      "   ------------------------------------- -- 14/15 [pytorch-lightning]\n",
      "   ---------------------------------------- 15/15 [pytorch-lightning]\n",
      "\n",
      "Successfully installed PyYAML-6.0.3 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 async-timeout-5.0.1 attrs-25.4.0 frozenlist-1.8.0 idna-3.11 lightning-utilities-0.15.2 multidict-6.7.0 propcache-0.4.1 pytorch-lightning-2.5.6 torchmetrics-1.8.2 tqdm-4.67.1 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install pytorch-lightning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-metric-learning\n",
      "  Using cached pytorch_metric_learning-2.9.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from pytorch-metric-learning) (2.1.2)\n",
      "Collecting scikit-learn (from pytorch-metric-learning)\n",
      "  Using cached scikit_learn-1.7.2-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from pytorch-metric-learning) (2.9.1+cu126)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from pytorch-metric-learning) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from torch>=1.6.0->pytorch-metric-learning) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from torch>=1.6.0->pytorch-metric-learning) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from torch>=1.6.0->pytorch-metric-learning) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from sympy>=1.13.3->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (2.1.5)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn->pytorch-metric-learning)\n",
      "  Using cached scipy-1.15.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->pytorch-metric-learning)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->pytorch-metric-learning)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from tqdm->pytorch-metric-learning) (0.4.6)\n",
      "Downloading pytorch_metric_learning-2.9.0-py3-none-any.whl (127 kB)\n",
      "Downloading scikit_learn-1.7.2-cp310-cp310-win_amd64.whl (8.9 MB)\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 4.7/8.9 MB 25.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.7/8.9 MB 25.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.9/8.9 MB 22.1 MB/s  0:00:00\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading scipy-1.15.3-cp310-cp310-win_amd64.whl (41.3 MB)\n",
      "   ---------------------------------------- 0.0/41.3 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 6.3/41.3 MB 32.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 12.8/41.3 MB 32.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 19.7/41.3 MB 32.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 26.7/41.3 MB 33.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 33.3/41.3 MB 33.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 40.1/41.3 MB 33.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.3/41.3 MB 31.6 MB/s  0:00:01\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, pytorch-metric-learning\n",
      "\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   -------- ------------------------------- 1/5 [scipy]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [scikit-learn]\n",
      "   -------------------------------- ------- 4/5 [pytorch-metric-learning]\n",
      "   -------------------------------- ------- 4/5 [pytorch-metric-learning]\n",
      "   ---------------------------------------- 5/5 [pytorch-metric-learning]\n",
      "\n",
      "Successfully installed joblib-1.5.2 pytorch-metric-learning-2.9.0 scikit-learn-1.7.2 scipy-1.15.3 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install pytorch-metric-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.1.2-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from xgboost) (2.1.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\envs\\test_env\\lib\\site-packages (from xgboost) (1.15.3)\n",
      "Downloading xgboost-3.1.2-py3-none-win_amd64.whl (72.0 MB)\n",
      "   ---------------------------------------- 0.0/72.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 6.0/72.0 MB 30.7 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 12.6/72.0 MB 31.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 18.6/72.0 MB 30.9 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 24.9/72.0 MB 30.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 32.5/72.0 MB 32.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 39.8/72.0 MB 33.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 47.7/72.0 MB 33.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 55.6/72.0 MB 34.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 64.2/72.0 MB 35.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 35.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 35.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 72.0/72.0 MB 29.8 MB/s  0:00:02\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# NOTEBOOK: Siamese Triplet + FC + XGBoost + Embedding visuals per epoch\n",
    "# ================================================================\n",
    "# 0) IMPORTS\n",
    "# ================================================================\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_metric_learning.losses import TripletMarginLoss\n",
    "from pytorch_metric_learning.miners import TripletMarginMiner\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Ajustes de visualizaci√≥n\n",
    "sns.set()\n",
    "plt.rcParams[\"figure.figsize\"] = (9,6)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 1) DATA TRANSFORMS + DATASET\n",
    "# ================================================================\n",
    "DATASET_ROOT = r\"D:\\DEEp\\deep\\dataset_split\"\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(os.path.join(DATASET_ROOT, \"train\"), transform=train_transform)\n",
    "val_dataset   = datasets.ImageFolder(os.path.join(DATASET_ROOT, \"val\"),   transform=test_transform)\n",
    "test_dataset  = datasets.ImageFolder(os.path.join(DATASET_ROOT, \"test\"),  transform=test_transform)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Clases detectadas:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 2) SIAMESE NETWORK\n",
    "# ================================================================\n",
    "class SiameseBackbone(nn.Module):\n",
    "    def __init__(self, embedding_size=128):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet50(weights=\"DEFAULT\")\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "siamese = SiameseBackbone().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "c:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA RTX 6000 Ada Generation') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | SiameseBackbone    | 23.8 M | train\n",
      "1 | miner     | TripletMarginMiner | 0      | train\n",
      "2 | loss_func | TripletMarginLoss  | 0      | train\n",
      "---------------------------------------------------------\n",
      "23.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.8 M    Total params\n",
      "95.081    Total estimated model params size (MB)\n",
      "157       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:428: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Please call `iter(combined_loader)` first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:49\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     48\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(*args, **kwargs)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:598\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    593\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    594\u001b[39m     ckpt_path,\n\u001b[32m    595\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    596\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    597\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m \u001b[38;5;28mself\u001b[39m._run(model, ckpt_path=ckpt_path)\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1011\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1008\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1011\u001b[39m results = \u001b[38;5;28mself\u001b[39m._run_stage()\n\u001b[32m   1013\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1055\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m     \u001b[38;5;28mself\u001b[39m.fit_loop.run()\n\u001b[32m   1056\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:208\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     \u001b[38;5;28mself\u001b[39m.setup_data()\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:275\u001b[39m, in \u001b[36m_FitLoop.setup_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28mself\u001b[39m._data_fetcher.setup(combined_loader)\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m._data_fetcher)  \u001b[38;5;66;03m# creates the iterator inside the fetcher\u001b[39;00m\n\u001b[32m    276\u001b[39m max_batches = sized_len(combined_loader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:105\u001b[39m, in \u001b[36m_PrefetchDataFetcher.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33m_PrefetchDataFetcher\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m    106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    107\u001b[39m         \u001b[38;5;66;03m# ignore pre-fetching, it's not necessary\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:52\u001b[39m, in \u001b[36m_DataFetcher.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33m_DataFetcher\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28mself\u001b[39m.iterator = \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m.combined_loader)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28mself\u001b[39m.reset()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:351\u001b[39m, in \u001b[36mCombinedLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    350\u001b[39m iterator = \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;28mself\u001b[39m.flattened, \u001b[38;5;28mself\u001b[39m._limits)\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[32m    352\u001b[39m \u001b[38;5;28mself\u001b[39m._iterator = iterator\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:92\u001b[39m, in \u001b[36m_MaxSizeCycle.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Self:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m     93\u001b[39m     \u001b[38;5;28mself\u001b[39m._consumed = [\u001b[38;5;28;01mFalse\u001b[39;00m] * \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterables)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:43\u001b[39m, in \u001b[36m_ModeIterator.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Self:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28mself\u001b[39m.iterators = [\u001b[38;5;28miter\u001b[39m(iterable) \u001b[38;5;28;01mfor\u001b[39;00m iterable \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iterables]\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m._idx = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:493\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_iterator()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:424\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    423\u001b[39m \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1171\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1170\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1171\u001b[39m w.start()\n\u001b[32m   1172\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\multiprocessing\\process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28mself\u001b[39m._Popen(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\multiprocessing\\context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_context.get_context().Process._Popen(process_obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\multiprocessing\\context.py:337\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Popen(process_obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     94\u001b[39m     reduction.dump(prep_data, to_child)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     reduction.dump(process_obj, to_child)\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\multiprocessing\\reduction.py:60\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, file, protocol)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m ForkingPickler(file, protocol).dump(obj)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m torch.optim.Adam(\u001b[38;5;28mself\u001b[39m.parameters(), lr=\u001b[38;5;28mself\u001b[39m.lr)\n\u001b[32m     29\u001b[39m triplet_trainer = pl.Trainer(max_epochs=\u001b[32m5\u001b[39m, accelerator=device, log_every_n_steps=\u001b[32m20\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m triplet_trainer.fit(TripletLightning(siamese), train_loader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:560\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    558\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m call._call_and_handle_interrupt(\n\u001b[32m    561\u001b[39m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[32m    562\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:62\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     60\u001b[39m signal.signal(signal.SIGINT, signal.SIG_IGN)\n\u001b[32m     61\u001b[39m _interrupt(trainer, exception)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m trainer._teardown()\n\u001b[32m     63\u001b[39m launcher = trainer.strategy.launcher\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1038\u001b[39m, in \u001b[36mTrainer._teardown\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1036\u001b[39m \u001b[38;5;66;03m# loop should never be `None` here but it can because we don't know the trainer stage with `ddp_spawn`\u001b[39;00m\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     loop.teardown()\n\u001b[32m   1039\u001b[39m \u001b[38;5;28mself\u001b[39m._logger_connector.teardown()\n\u001b[32m   1040\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.teardown()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:505\u001b[39m, in \u001b[36m_FitLoop.teardown\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mteardown\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    504\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m         \u001b[38;5;28mself\u001b[39m._data_fetcher.teardown()\n\u001b[32m    506\u001b[39m         \u001b[38;5;28mself\u001b[39m._data_fetcher = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    507\u001b[39m     \u001b[38;5;28mself\u001b[39m.epoch_loop.teardown()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:80\u001b[39m, in \u001b[36m_DataFetcher.teardown\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mteardown\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28mself\u001b[39m.reset()\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._combined_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     82\u001b[39m         \u001b[38;5;28mself\u001b[39m._combined_loader.reset()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:142\u001b[39m, in \u001b[36m_PrefetchDataFetcher.reset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28msuper\u001b[39m().reset()\n\u001b[32m    143\u001b[39m     \u001b[38;5;28mself\u001b[39m.batches = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:76\u001b[39m, in \u001b[36m_DataFetcher.reset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# teardown calls `reset()`, and if it happens early, `combined_loader` can still be None\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._combined_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28mself\u001b[39m.length = sized_len(\u001b[38;5;28mself\u001b[39m.combined_loader)\n\u001b[32m     77\u001b[39m     \u001b[38;5;28mself\u001b[39m.done = \u001b[38;5;28mself\u001b[39m.length == \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\lightning_fabric\\utilities\\data.py:52\u001b[39m, in \u001b[36msized_len\u001b[39m\u001b[34m(dataloader)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Try to get the length of an object, return ``None`` otherwise.\"\"\"\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m# try getting the length\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     length = \u001b[38;5;28mlen\u001b[39m(dataloader)  \u001b[38;5;66;03m# type: ignore [arg-type]\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m):\n\u001b[32m     54\u001b[39m     length = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I21326\\.conda\\envs\\VS\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:358\u001b[39m, in \u001b[36mCombinedLoader.__len__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    356\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute the number of batches.\"\"\"\u001b[39;00m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPlease call `iter(combined_loader)` first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._iterator)\n",
      "\u001b[31mRuntimeError\u001b[39m: Please call `iter(combined_loader)` first."
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 3) PYTORCH LIGHTNING TRAINER PARA TRIPLET LOSS\n",
    "# ================================================================\n",
    "class TripletLightning(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-4, margin=0.2):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.miner = TripletMarginMiner(margin=margin, type_of_triplets=\"semihard\")\n",
    "        self.loss_func = TripletMarginLoss(margin=margin)\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        embeddings = self(imgs)\n",
    "        hard_pairs = self.miner(embeddings, labels)\n",
    "        loss = self.loss_func(embeddings, labels, hard_pairs)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "lightning_model = TripletLightning(siamese)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=5, accelerator=device, log_every_n_steps=20)\n",
    "trainer.fit(lightning_model, train_loader)\n",
    "\n",
    "# ‚≠ê GUARDAR MODELO SIAM√âS\n",
    "torch.save(siamese.state_dict(), \"siamese_triplet.pth\")\n",
    "print(\"Modelo siam√©s guardado como siamese_triplet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 4) CLASIFICADOR FC (BACKBONE CONGELADO)\n",
    "# ================================================================\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, backbone, emb_size, num_classes):\n",
    "        super().__init__()\n",
    "        for p in backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.fc = nn.Linear(emb_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            emb = self.backbone(x)\n",
    "        return self.fc(emb)\n",
    "\n",
    "classifier = Classifier(siamese, 128, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.fc.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(8):\n",
    "    classifier.train()\n",
    "    running_loss = 0\n",
    "    for img, lbl in train_loader:\n",
    "        img, lbl = img.to(device), lbl.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = classifier(img)\n",
    "        loss = criterion(out, lbl)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# ‚≠ê GUARDAR CLASIFICADOR FC\n",
    "torch.save(classifier.state_dict(), \"classifier_fc.pth\")\n",
    "print(\"Clasificador FC guardado como classifier_fc.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 5) EVALUACI√ìN DEL CLASIFICADOR FC\n",
    "# ================================================================\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            out = model(x)\n",
    "            p = out.argmax(1).cpu().numpy()\n",
    "            preds.extend(p)\n",
    "            labels.extend(y.numpy())\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1  = f1_score(labels, preds, average=\"macro\")\n",
    "    return acc, f1, labels, preds\n",
    "\n",
    "acc, f1, y_true, y_pred = evaluate(classifier, test_loader)\n",
    "\n",
    "print(\"Accuracy FC:\", acc)\n",
    "print(\"F1-score FC:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 6) EXTRAER EMBEDDINGS (PARA XGBOOST)\n",
    "# ================================================================\n",
    "def extract_embeddings(model, loader):\n",
    "    model.eval()\n",
    "    all_emb, all_lbl = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            emb = model(x).cpu().numpy()\n",
    "            all_emb.append(emb)\n",
    "            all_lbl.append(y.numpy())\n",
    "\n",
    "    return np.vstack(all_emb), np.hstack(all_lbl)\n",
    "\n",
    "train_emb, train_lbl = extract_embeddings(siamese, train_loader)\n",
    "test_emb,  test_lbl  = extract_embeddings(siamese, test_loader)\n",
    "\n",
    "print(\"Embeddings shape:\", train_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 6.1) VISUALIZAR EMBEDDINGS EN 2D\n",
    "# ================================================================\n",
    "def plot_embeddings(embeddings, labels, method=\"pca\", title=\"Embeddings 2D\"):\n",
    "    \"\"\"\n",
    "    embeddings: np.array (N, embedding_dim)\n",
    "    labels: np.array (N,)\n",
    "    method: \"pca\" o \"tsne\"\n",
    "    \"\"\"\n",
    "    if method.lower() == \"tsne\":\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    else:\n",
    "        reducer = PCA(n_components=2)\n",
    "        \n",
    "    emb_2d = reducer.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    num_classes = len(np.unique(labels))\n",
    "    for c in range(num_classes):\n",
    "        idx = labels == c\n",
    "        plt.scatter(emb_2d[idx, 0], emb_2d[idx, 1], label=str(c), alpha=0.6, s=15)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Dim 1\")\n",
    "    plt.ylabel(\"Dim 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar embeddings de entrenamiento\n",
    "plot_embeddings(train_emb, train_lbl, method=\"pca\", title=\"Embeddings PCA - Train\")\n",
    "\n",
    "# Visualizar embeddings de test\n",
    "plot_embeddings(test_emb, test_lbl, method=\"pca\", title=\"Embeddings PCA - Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 7) XGBOOST PARA CLASIFICAR EMBEDDINGS\n",
    "# ================================================================\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "\n",
    "xgb.fit(train_emb, train_lbl)\n",
    "xgb_preds = xgb.predict(test_emb)\n",
    "\n",
    "acc_xgb = accuracy_score(test_lbl, xgb_preds)\n",
    "f1_xgb  = f1_score(test_lbl, xgb_preds, average=\"macro\")\n",
    "\n",
    "print(\"üéØ XGBoost Accuracy:\", acc_xgb)\n",
    "print(\"üî• XGBoost F1-score:\", f1_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 8) MATRIZ DE CONFUSI√ìN XGBOOST\n",
    "# ================================================================\n",
    "cm = confusion_matrix(test_lbl, xgb_preds)\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(cm, annot=False, cmap=\"Greens\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(\"Matriz de Confusi√≥n - XGBoost\")\n",
    "plt.xlabel(\"Predicci√≥n\")\n",
    "plt.ylabel(\"Verdad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 9) GUARDAR MODELOS\n",
    "# ================================================================\n",
    "torch.save(siamese.state_dict(), \"siamese_triplet_backbone.pth\")\n",
    "torch.save(classifier.state_dict(), \"classifier_fc_triplet.pth\")\n",
    "xgb.save_model(\"xgboost_triplet.json\")\n",
    "print(\"‚úÖ Modelos guardados en la carpeta del notebook\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
